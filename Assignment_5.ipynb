{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fd097a40",
      "metadata": {
        "id": "fd097a40"
      },
      "source": [
        "### Article Generation using N-grams"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbfd8420",
      "metadata": {
        "id": "bbfd8420"
      },
      "source": [
        "You have to write the urdu article using ngrams (1 grams to 5 grams). So, in short your output must be of 5 paragraphs, the first one is generated using unigram, second one is generated using bigram and so on.\n",
        "\n",
        "Input: Your input is the seed sentence. E.g. first 3 to 4 words of the paragraph.\n",
        "\n",
        "Output: Your output is the consist of 5 paragraphs for each n gram, each of 200 words.\n",
        "\n",
        "You have to make N-gram model using the provided dataset. Dataset can be downloaded from  https://www.kaggle.com/datasets/saurabhshahane/urdu-news-dataset\n",
        "\n",
        "You have to use all News Text column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bb709115",
      "metadata": {
        "id": "bb709115"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Urdu N-gram generator (1-gram to 5-gram) using a local CSV file.\n",
        "\n",
        "Dependencies:\n",
        "  pip install pandas tqdm\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "import random\n",
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "WORD_RE = re.compile(r\"[\\u0600-\\u06FF]+|[A-Za-z0-9]+|[،؛؟.!]+\", flags=re.UNICODE)\n",
        "PUNCT = set(\"،؛؟.!؟\")\n",
        "\n",
        "def tokenize(text):\n",
        "    return WORD_RE.findall(text)\n",
        "\n",
        "def detokenize(tokens):\n",
        "    out = []\n",
        "    for t in tokens:\n",
        "        if out and t in PUNCT:\n",
        "            out[-1] = out[-1] + t\n",
        "        else:\n",
        "            out.append(t)\n",
        "    return \" \".join(out)\n",
        "\n",
        "def load_dataset(csv_path):\n",
        "    df = pd.read_csv(csv_path, encoding=\"utf-8\", low_memory=False)\n",
        "    text_cols = []\n",
        "    for col in df.columns:\n",
        "        sample = df[col].astype(str).dropna().head(200).str.strip()\n",
        "        if sample.str.contains(r\"[\\u0600-\\u06FF]\", regex=True).sum() > 0:\n",
        "            text_cols.append(col)\n",
        "    if not text_cols:\n",
        "        raise ValueError(\"No Urdu text column found in dataset.\")\n",
        "    series = df[text_cols].astype(str).fillna(\"\").agg(\" \".join, axis=1)\n",
        "    return series[series.str.strip().str.len() > 20].tolist()\n",
        "\n",
        "def preprocess_corpus(raw_texts):\n",
        "    tokenized = []\n",
        "    for t in tqdm(raw_texts, desc=\"Tokenizing corpus\"):\n",
        "        toks = tokenize(str(t).strip())\n",
        "        if toks:\n",
        "            tokenized.append(toks)\n",
        "    return tokenized\n",
        "\n",
        "def build_ngrams(corpus_tokens_list, max_n=5):\n",
        "    models = {n: defaultdict(Counter) for n in range(1, max_n + 1)}\n",
        "    for tokens in tqdm(corpus_tokens_list, desc=\"Building n-grams\"):\n",
        "        L = len(tokens)\n",
        "        for i in range(L):\n",
        "            for n in range(1, max_n + 1):\n",
        "                if i + n <= L:\n",
        "                    ngram = tuple(tokens[i:i + n])\n",
        "                    context = () if n == 1 else ngram[:-1]\n",
        "                    word = ngram[-1]\n",
        "                    models[n][context][word] += 1\n",
        "    return models\n",
        "\n",
        "def sample_next(counter):\n",
        "    if not counter:  # Handle empty counter\n",
        "        return \"\"\n",
        "    words, freqs = zip(*counter.items())\n",
        "    total = sum(freqs)\n",
        "    if total == 0: #Handle zero total frequency\n",
        "        return \"\"\n",
        "    return random.choices(words, weights=[f/total for f in freqs], k=1)[0]\n",
        "\n",
        "def generate_paragraph(models, order, seed_tokens, target_words=200):\n",
        "    out = list(seed_tokens)\n",
        "    while len(out) < target_words:\n",
        "        placed = False\n",
        "        for context_len in reversed(range(0, order)):\n",
        "            context = tuple(out[-context_len:]) if context_len > 0 else ()\n",
        "            counter = models[context_len + 1].get(context)\n",
        "            if counter:\n",
        "                next_word = sample_next(counter)\n",
        "                if next_word:\n",
        "                    out.append(next_word)\n",
        "                    placed = True\n",
        "                    break\n",
        "        if not placed:\n",
        "            next_word = sample_next(models[1][()])\n",
        "            if next_word:\n",
        "                out.append(next_word)\n",
        "            else: # Break if no word can be sampled even from unigram\n",
        "                break\n",
        "    return detokenize(out[:target_words])\n",
        "\n",
        "# Modified main function to accept arguments directly\n",
        "def main(csv_path, seed, words=200, output=\"generated_urdu_paragraphs.txt\"):\n",
        "    raw_texts = load_dataset(csv_path)\n",
        "    tokenized_texts = preprocess_corpus(raw_texts)\n",
        "    models = build_ngrams(tokenized_texts, max_n=5)\n",
        "    seed_tokens = tokenize(seed)\n",
        "    outputs = []\n",
        "    for n in range(1, 6):\n",
        "        paragraph = generate_paragraph(models, n, seed_tokens, target_words=words)\n",
        "        outputs.append((n, paragraph))\n",
        "    with open(output, \"w\", encoding=\"utf-8\") as f:\n",
        "        for n, p in outputs:\n",
        "            f.write(f\"--- {n}-gram paragraph ---\\n\")\n",
        "            f.write(p + \"\\n\\n\")\n",
        "    print(\"Saved generated paragraphs to\", output)\n",
        "\n",
        "# Example usage within the notebook:\n",
        "# main(csv_path=\"/content/urdu-news-dataset-1M.csv\", seed=\"یہ ایک مثال\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "608aa542",
        "outputId": "502c13ae-2604-4a05-b213-4694d72e2ee4"
      },
      "source": [
        "# Example usage within the notebook:\n",
        "main(csv_path=\"/content/urdu-news-dataset-1M.csv\", seed=\"یہ ایک مثال\")"
      ],
      "id": "608aa542",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Tokenizing corpus: 100%|██████████| 46216/46216 [00:06<00:00, 6813.61it/s] \n",
            "Building n-grams: 100%|██████████| 46216/46216 [03:03<00:00, 252.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved generated paragraphs to generated_urdu_paragraphs.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79601988",
      "metadata": {
        "id": "79601988"
      },
      "source": [
        "### Classify language out of the list given below using just stop words. Remove punctuations, make it lower."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2095f6b3",
        "outputId": "81f4d2e5-952f-48ea-d482-bbd18c12fe0f"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "id": "2095f6b3",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fa286b84",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa286b84",
        "outputId": "7a2b660d-190a-4c7e-8b6c-3a8391131eeb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['albanian',\n",
              " 'arabic',\n",
              " 'azerbaijani',\n",
              " 'basque',\n",
              " 'belarusian',\n",
              " 'bengali',\n",
              " 'catalan',\n",
              " 'chinese',\n",
              " 'danish',\n",
              " 'dutch',\n",
              " 'english',\n",
              " 'finnish',\n",
              " 'french',\n",
              " 'german',\n",
              " 'greek',\n",
              " 'hebrew',\n",
              " 'hinglish',\n",
              " 'hungarian',\n",
              " 'indonesian',\n",
              " 'italian',\n",
              " 'kazakh',\n",
              " 'nepali',\n",
              " 'norwegian',\n",
              " 'portuguese',\n",
              " 'romanian',\n",
              " 'russian',\n",
              " 'slovene',\n",
              " 'spanish',\n",
              " 'swedish',\n",
              " 'tajik',\n",
              " 'tamil',\n",
              " 'turkish']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "stopwords.fileids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1bdad25b",
      "metadata": {
        "id": "1bdad25b"
      },
      "outputs": [],
      "source": [
        "Test=\"An article is qualunque member van un class of dedicated words naquele estão used with noun phrases per mark the identifiability of the referents of the noun phrases\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        },
        "id": "89bdd923",
        "outputId": "4dffbfd1-0835-47bf-a441-44f2ed10a925"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "def classify_language(text, languages):\n",
        "    \"\"\"Classifies the language of a text based on stopwords.\"\"\"\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    words = text.split()\n",
        "\n",
        "    language_scores = {}\n",
        "    for lang in languages:\n",
        "        try:\n",
        "            stop_words = set(stopwords.words(lang))\n",
        "            score = len(set(words) & stop_words)\n",
        "            language_scores[lang] = score\n",
        "        except OSError:\n",
        "            # Handle languages for which stopwords are not available\n",
        "            language_scores[lang] = 0\n",
        "    return language_scores\n",
        "\n",
        "# Get the list of available languages for stopwords\n",
        "available_languages = stopwords.fileids()\n",
        "\n",
        "# Test the function with the provided text\n",
        "language_scores = classify_language(Test, available_languages)\n",
        "\n",
        "display(language_scores)"
      ],
      "id": "89bdd923",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'albanian': 1,\n",
              " 'arabic': 0,\n",
              " 'azerbaijani': 1,\n",
              " 'basque': 0,\n",
              " 'belarusian': 0,\n",
              " 'bengali': 0,\n",
              " 'catalan': 3,\n",
              " 'chinese': 0,\n",
              " 'danish': 0,\n",
              " 'dutch': 3,\n",
              " 'english': 5,\n",
              " 'finnish': 0,\n",
              " 'french': 1,\n",
              " 'german': 1,\n",
              " 'greek': 0,\n",
              " 'hebrew': 0,\n",
              " 'hinglish': 8,\n",
              " 'hungarian': 1,\n",
              " 'indonesian': 1,\n",
              " 'italian': 2,\n",
              " 'kazakh': 0,\n",
              " 'nepali': 0,\n",
              " 'norwegian': 0,\n",
              " 'portuguese': 1,\n",
              " 'romanian': 1,\n",
              " 'russian': 0,\n",
              " 'slovene': 0,\n",
              " 'spanish': 1,\n",
              " 'swedish': 0,\n",
              " 'tajik': 0,\n",
              " 'tamil': 0,\n",
              " 'turkish': 0}"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43708fa5",
      "metadata": {
        "id": "43708fa5"
      },
      "source": [
        "### Rule Based Roman Urdu Text Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e1f761e",
      "metadata": {
        "id": "2e1f761e"
      },
      "source": [
        "Roman Urdu lacks standard lexicon and usually many spelling variations exist for a given word, e.g., the word zindagi (life) is also written as zindagee, zindagy, zaindagee and zndagi. So, in this question you have to Normalize Roman Urdu words using the following Rules given in the attached Pdf. Your Code works for a complete Sentence or multiple sentences.\n",
        "\n",
        "For Example: zaroori, zaruri, zarori map to the 'zrory'. So zrory becomes the correct word for all representations mentioned above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "dd7e7159",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd7e7159",
        "outputId": "b4a23f0c-92df-4e0e-95d5-3846191bf1ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "zrory zrory zrory zindagy zindagy zaendagy zndagy\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Function to normalize text based on the given rules\n",
        "def normalize_roman_urdu(text):\n",
        "    rules = [\n",
        "        # Rule #: (pattern, replacement, flags)\n",
        "        (r'ain$', 'ein'),                 # 1\n",
        "        (r'(?<!^)(ar)', 'r'),             # 2 (not at start)\n",
        "        (r'ai', 'ae'),                    # 3\n",
        "        (r'i[yY]+', 'i'),                 # 4 multiple y's\n",
        "        (r'ay$', 'e'),                    # 5\n",
        "        (r'ih[hH]+', 'eh'),               # 6\n",
        "        (r'ey$', 'e'),                    # 7\n",
        "        (r's+', 's'),                     # 8\n",
        "        (r'ie$', 'e'),                    # 9\n",
        "        (r'ry(?!$)', 'ri'),               # 10 (not at end)\n",
        "        (r'^es', 'is'),                   # 11 (start)\n",
        "        (r'sy$', 'si'),                   # 12 (end)\n",
        "        (r'a+', 'a'),                     # 13\n",
        "        (r'ty(?!$)', 'ti'),               # 14 (not at end)\n",
        "        (r'j+', 'j'),                     # 15\n",
        "        (r'o+', 'o'),                     # 16\n",
        "        (r'e{2,}', 'i'),                  # 17 multiple e's\n",
        "        (r'(?<=[a-zA-Z])i$', 'y'),        # 18 i→y if preceded by letter\n",
        "        (r'd+', 'd'),                     # 19\n",
        "        (r'u', 'o'),                      # 20\n",
        "        (r'(?<=\\w)h', '')                  # 21 remove h if preceded by letter\n",
        "    ]\n",
        "\n",
        "    for pattern, repl in rules:\n",
        "        text = re.sub(pattern, repl, text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Example\n",
        "sentence = \"zaroori zaruri zarori zindagee zindagy zaindagee zndagi\"\n",
        "words = sentence.split()\n",
        "normalized_words = [normalize_roman_urdu(word) for word in words]\n",
        "\n",
        "print(\" \".join(normalized_words))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "142c871a",
      "metadata": {
        "id": "142c871a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NGzrC1wQbwOP"
      },
      "id": "NGzrC1wQbwOP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BV_9fbS-fKJ-"
      },
      "id": "BV_9fbS-fKJ-",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}